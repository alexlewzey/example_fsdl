{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "lr = 0.02\n",
    "batch_size = 65536\n",
    "# batch_size = None\n",
    "quick_run = {\n",
    "'max_epochs': None,\n",
    "# 'limit_train_batches': 0.1,\n",
    "# 'limit_val_batches': 0.1,\n",
    "# 'limit_test_batches': 0.1,\n",
    "}\n",
    "fast_dev_run_kwargs = {'fast_dev_run': True, 'enable_checkpointing': False}\n",
    "overfit_batches_kwargs = {'overfit_batches': True, 'enable_checkpointing': False}\n",
    "large_model = {'precision': \"16-mixed\"}\n",
    "grad_accum = {'accumulate_grad_batches': 7}\n",
    "resume_training = {'ckpt_path': 'path/to/ckpt'}\n",
    "arcitecture_name = 'autoencoder' # convencoder, autoencoder\n",
    "experiemnt_name = \"mnist-autoencoder\"\n",
    "run_name = arcitecture_name\n",
    "dir_artifacts = dir_artifacts/arcitecture_name\n",
    "dir_artifacts.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_in, h1, h2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(n_in, h1), nn.ReLU(), nn.Linear(h1, h2))\n",
    "        self.decoder = nn.Sequential(nn.Linear(h2, h1), nn.ReLU(), nn.Linear(h1, n_in))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 28*28)\n",
    "        encoded = self.encoder(x)\n",
    "        x_hat = self.decoder(encoded)\n",
    "        return x_hat\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), # Assuming MNIST images are 1x28x28\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 3, 7) # This will output 3 numbers\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 8, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid() # Using Sigmoid for output because MNIST pixels are in range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 28, 28)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x.view(x.size(0),1,28,28))\n",
    "\n",
    "\n",
    "class MNISTAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, model, lr:float=2e-2):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_func = F.mse_loss\n",
    "        self.lr = lr\n",
    "        self.example_input_array = torch.randn(5, 784)\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _step(self, batch, idx, set_name: str):\n",
    "        x,y=batch\n",
    "        x_hat = self(x)\n",
    "        loss = self.loss_func(x_hat.view(x.size(0), -1), x)\n",
    "        self.log(f'{set_name}_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, idx):\n",
    "        return self._step(batch, idx, 'train')\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        return self._step(batch, idx, 'valid')\n",
    "\n",
    "    def test_step(self, batch, idx):\n",
    "        return self._step(batch, idx, 'test')\n",
    "\n",
    "    def predict_step(self, batch, idx,  dataloader_idx=0):\n",
    "        return self(batch[0])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(params=self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('file://' + dir_mlruns.as_posix())\n",
    "mlflow.set_experiment(experiment_name=experiemnt_name)\n",
    "mlflow.pytorch.autolog()\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    data = MNISTDataModule(dir_mnist.as_posix())\n",
    "    arcitectures = {\n",
    "        'autoencoder': AutoEncoder(784, 64, 3),\n",
    "        'convencoder': ConvAutoEncoder(),\n",
    "    }\n",
    "    arcitecture = arcitectures[arcitecture_name]\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(every_n_epochs=2),\n",
    "        EarlyStopping(monitor=\"valid_loss\"),\n",
    "        StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "    ]\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=experiemnt_name,\n",
    "        run_id=mlflow.active_run().info.run_id,\n",
    "        log_model=True,\n",
    "        tracking_uri=uri_mlruns\n",
    "    )\n",
    "    trainer = L.Trainer(callbacks=callbacks, logger=mlf_logger, **quick_run)\n",
    "    \n",
    "    model = MNISTAutoEncoder(model=arcitecture, lr=lr)\n",
    "    data = MNISTDataModule(dir_mnist.as_posix(), batch_size=batch_size)\n",
    "    trainer.fit(model, datamodule=data)\n",
    "\n",
    "    trainer.predict(model, datamodule=data)\n",
    "    trainer.test(model, data)\n",
    "    mlflow.log_artifacts(dir_artifacts.as_posix())\n",
    "\n",
    "mlflow.pytorch.autolog(disable=True)\n",
    "launch_mlflow_ui(uri=uri_mlruns, run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.setup('predict')\n",
    "x = data.ds_predict.x\n",
    "y = data.ds_predict.y\n",
    "x_hat = model(x)\n",
    "\n",
    "encoded = model.model.encode(x).detach().numpy()\n",
    "df = pd.DataFrame(encoded)\n",
    "columns = ['e0', 'e1', 'e2']\n",
    "df.columns = columns\n",
    "df['lbl'] = y.detach().numpy().astype(str)\n",
    "\n",
    "x,y,x_hat = map(lambda t:t.detach().numpy(), (x,y,x_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 16\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(x[i].reshape(28,28), cmap='gray')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(x_hat[i].reshape(28,28), cmap='gray')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = px.scatter_3d(df, *columns, color='lbl')\n",
    "f.update_traces(marker_size=3)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "df = pd.DataFrame(pca.fit_transform(x))\n",
    "columns = ['e0', 'e1', 'e2']\n",
    "df.columns = columns\n",
    "df['lbl'] = y.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = px.scatter_3d(df, *columns, color='lbl')\n",
    "f.update_traces(marker_size=3)\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
